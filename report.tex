\documentclass{amsart}
\usepackage{geometry}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{hyperref}

\newcommand{\R}{\mathbb{R}}

\title{Nonnegative Matrix Factorization}
\author{Sofía Llavayol}
\date{\today}
%\email{so.llavayol@gmail.com}

\begin{document}

\begin{abstract}
    This document presents the final project for the course \textit{Numerical Linear Algebra for Statistical Learning} at Universidad de la República, Uruguay. It outlines the fundamental concepts of Nonnegative Matrix Factorization based on the reference \cite{G}, and includes selected experiments implemented in Python to illustrate key ideas.
\end{abstract}


\maketitle

\section{Introduction}

{\it Nonnegative matrix factorization (NMF)} is an easily interpretable {\it linear dimensionality reduction (LDR)} technique for nonnegative data. We first introduce the general concept of LDR, followed by a more detailed discussion of NMF.

\subsection{LDR techniques for Data
Analysis}

Extracting the underlying structure within data sets is one of the central problems in data science, and numerous techniques exist to perform this task. One of the oldest approaches is LDR. The idea of LDR is to represent each data point as a linear combination of a small number of basis elements.

Mathematically, given a dataset of $n$ data points $x_1, \ldots, x_n \in\R^m$, LDR looks for $r\ll\min\{m,n\}$ basis vectors $w_1, \ldots, w_r \in\R^m$ such that each data point $x_j$ is well-approximated by a linear combination of these basis vectors:
\[
    x_j\approx w_1\cdot h_{1j} +\cdots +w_r\cdot h_{rj}=
    \begin{bmatrix}
        w_1 \cdots w_r
    \end{bmatrix}
    \begin{bmatrix}
        h_{1j}\\
        \vdots\\
        h_{rj}
    \end{bmatrix}= W h_j,
\]
for some $h_j= \begin{bmatrix} h_{1j}, \ldots, h_{rj} \end{bmatrix}^T \in\R^r$.

Note that this is equivalent to {\it low-rank matrix approximation (LRMA)} --that is, expressing $X \approx WH$ where
\begin{itemize}
    \item each column of $X\in\R^{m\times n}$ is a data point, $X(:,j)=x_j$;
    \item each column of $W\in\R^{m\times r}$ is a basis element, $W(:,j)=w_j$;
    \item each column of $H\in\R^{r\times n}$ contains the coordinates of a data point $x_j$ in the basis $W$, $H(:,j)=h_j$.
\end{itemize}
Hence LDR provides a rank-$r$ approximation $WH$ of $X$, which can be written as:
\[
    \begin{bmatrix} x_1 \cdots x_n \end{bmatrix} \approx
    \begin{bmatrix} w_1 \cdots w_r \end{bmatrix}
    \begin{bmatrix} h_1 \cdots h_n \end{bmatrix}.
\]

\bigskip

In order to compute $W$ and $H$ given $X$ and $r$, one needs to define an error measure. For example, when $(W,H)$ minimizes the Frobenius norm
\[  
    \|X-WH\|_F^2 = \sum_{i,j} (X-WH)_{ij}^2,
\]
then LRMA is equivalent to {\it principal component analysis (PCA)}, which can be computed via the {\it singular value decomposition (SVD)}.

\bigskip

LRMA models are used to compress the data, filter the noise, reduce the computational effort for further manipulation of the data, or to directly identify hidden structure in a data set. Many variants of LRMA have been developed, and they differ in two key aspects: (1) the error measure can vary and should be chosen depending on the noise statistic assumed on the data,
(2) different constraints can be imposed on the factors $W$ and $H$.

\subsection{NMF, an LDR technique for nonnegative data}

Among LRMA models, nonnegative matrix factorization requires the factor matrices $W$ and $H$ to be componentwise nonnegative, which we denote $W\geq 0$ and $H\geq 0$. In Section~\ref{facial_feature_extraction}, we discuss an application where these nonnegativity constraints are natural and meaningful.

Formally, the NMF problem is defined as follows.

\bigskip

\noindent\fbox{
    \parbox{\textwidth}{
        {\bf NMF Problem.} Given a nonnegative matrix \( X \in \mathbb{R}^{m \times n} \), a factorization rank \( r \), and a distance measure between matrices \( d(\cdot, \cdot) \), solve
        \begin{equation}
            \label{optimization_problem}
            \min_{\substack{W \in \mathbb{R}^{m \times r} \\ H \in \mathbb{R}^{r \times n}}} d(X, WH) \quad \text{subject to } W \geq 0 \text{ and } H \geq 0.
        \end{equation}
    }%
}

\bigskip

In Section~\ref{NMF_algorithm}, we discuss an algorithm to approximately solve this problem in the case where the distance measure is induced by the Frobenius norm. An application of this algorithm to image processing is presented in Section~\ref{facial_feature_extraction}.

\section{Application on Facial Feature Extraction} \label{facial_feature_extraction}

$\cdots$

\section{Algorithm with multiplicative updates} \label{NMF_algorithm}

Let $X\in\R^{m\times n}$ and $r \ll\min\{m,n\}$ be given. In this section, we focus on the following constrained optimization problem
\begin{equation}
    \label{optimization_problem_frobenius}
    \min_{\substack{W \in \mathbb{R}^{m \times r} \\ H \in \mathbb{R}^{r \times n}}} f(W,H) \quad \text{subject to } W \geq 0 \text{ and } H \geq 0,
\end{equation}
where $f(W,H)= \frac{1}{2} \|X-WH\|_F^2$. Note that this problem is equivalent to \eqref{optimization_problem} for the case where $d(X,WH)= \|X-WH\|_F$. We will apply the KKT conditions, as described in Appendix~\ref{constrained_optimization_methods}.

\subsection{KKT conditions}

We will start by applying the conditions in \eqref{KKT_conditions} for the variables $W=(W_{ik})$. The constraints are given by $g_{ik}(W)= -W_{ik}\leq 0$, so the stationarity condition reads
\[
    0= \frac{\partial f}{\partial W_{ik}} -\sum_{} \lambda_{rs}\cdot \frac{\partial g_{rs}}{\partial W_{ik}} = \frac{\partial f}{\partial W_{ik}} +\lambda_{ik}= \left(\nabla_W f\right)_{ik} +\lambda_{ik}.
\]
Writing $\Lambda_W=(\lambda_{ik})$, we obtain
\[
    \Lambda_W= -\nabla_W f= -(X-WH)H^T= WHH^T -XH^T.
\]
Also, the condition $\lambda_{ik}\cdot g_{ik}(W)=0$ in matrix form reads
\[
    0= \Lambda_W\circ W= -\nabla_W f\circ W
\]
where $\circ$ is the component-wise product of two matrices.

Similarly, for the variables $H=(H_{kj})$, the constraints are $-H_{kj}\leq 0$, and applying the same reasoning yields
\[
    \Lambda_H= -\nabla_H f= -W^T(X-WH)= W^TWH -W^TX,
\]
\[
    0= \nabla_H f\circ H.
\]

To finish, adding the conditions $\Lambda_W\geq 0$ and $\Lambda_H\geq 0$, the KKT condition read
\begin{equation}
    \begin{cases}
        W \geq 0,\quad \nabla_W f = WHH^T - VH^T \geq 0,\quad W \circ \nabla_W f = 0, \\
        H \geq 0,\quad \nabla_H f = W^TWH - W^TV \geq 0,\quad H \circ \nabla_H f = 0.
    \end{cases}
\end{equation}
These conditions characterize first-order optimality for the constrained problem, and are satisfied at any local minimum.

\subsection{Multiplicative updates}




\appendix

\section{Constrained optimization methods}\label{constrained_optimization_methods}

In many practical optimization problems, the solution is required to satisfy certain constraints. This section introduces two fundamental approaches for handling constraints: the method of Lagrange multipliers for equality constraints, and the {\it Karush-Kuhn-Tucker (KKT)} conditions for inequality constraints.

\subsection{Lagrange multipliers}

Consider the following optimization problem with one equality constrain
\begin{equation}
    \label{lagrange_one_constrain}
    \min_{x,y} f(x,y) \quad \text{subject to } g(x,y)= 0.
\end{equation}
We assume that $f$ and $g$ have continuous first partial derivatives.

\bigskip

Suppose that the point $(x_0,y_0)$ satisfies  the constraint $g(x_0,y_0)=0$ and that the gradient $\nabla g(x_0,y_0)\neq 0$. Recall that the gradient $\nabla g(x_0,y_0)$ is orthogonal to the level set defined by $g(x,y)=0$. Therefore, if $f(x_0,y_0)$ is a minimum of the constrained problem \eqref{lagrange_one_constrain}, then the gradient $\nabla f(x_0,y_0)$ must be parallel to $\nabla g(x_0,y_0)$. Otherwise, one could move along the constraint set $g(x,y)=0$ in a direction that decreases $f$, contradicting the minimality of $f(x_0,y_0)$.

In summary, if $f(x_0,y_0)$ is a minimum of the constrained problem \eqref{lagrange_one_constrain} and $\nabla g(x_0,y_0)\neq 0$, then there exists $\lambda_0\in\R$ such that
\[
    \nabla f(x_0,y_0)= \lambda_0\cdot \nabla g(x_0,y_0).
\]

\bigskip

Defining the {\it Lagrange function} as
\[
    \mathcal{L}(x,y,\lambda)= f(x,y) -\lambda\cdot g(x,y).
\]
Then, the gradient of $\mathcal{L}$ is given by
\[
    \nabla\mathcal{L}(x,y,\lambda)=
    \Big(
        \nabla f(x,y) -\lambda\cdot \nabla g(x,y),\ \
        -g(x,y)
    \Big).
\]
Thus, the condition $\nabla\mathcal{L}(x_0,y_0,\lambda_0)=0$ encodes the necessary conditions for $(x_0,y_0)$ to be a solution of the constrained optimization problem \eqref{lagrange_one_constrain}, as discussed above.

\bigskip

To solve the original constrained optimization problem \eqref{lagrange_one_constrain}, we look for points $(x,y,\lambda)$ such that $\nabla\mathcal{L}(x,y,\lambda)=0$, that is to say
\[\begin{cases}
    \nabla f(x,y) -\lambda\cdot \nabla g(x,y)= 0\\
    g(x,y)= 0
\end{cases}\]
In other words, we reduce the problem to solving a system of equations given by the vanishing of the gradient of the Lagrange function. Any solution $(x_0,y_0,\lambda_0)$ of this system provides a candidate for a constrained extremum of $f$ subject to $g(x,y)=0$.

\bigskip

\paragraph{\bf Multiple equality constraints}

The method described above naturally extends to optimization problems with multiple equality constraints. Suppose we have $M$ constraints $g_i(x,y)=0$, for $i=1,\ldots, M$. We define the Lagrange function as
\[
    \mathcal{L}(x,y,\lambda_1,\ldots,\lambda_M)= f(x,y) -\sum_{i=1}^M \lambda_i\cdot g_i(x,y).
\]
To find candidate solutions, we again look for points such that $\nabla\mathcal{L}(x,y,\lambda_1,\ldots,\lambda_M)=0$.

\subsection{Karush-Kuhn-Tucker (KKT) conditions}

If we now modify the constraint in \eqref{lagrange_one_constrain} to an inequality constraint, namely $g(x,y)\leq 0$, a similar principle applies. However, since the feasible set may include boundary and interior points, we must refine the conditions under which a point can be optimal. We now look for points $(x,y,\lambda)$ such that
\[\begin{cases}
    \nabla f(x,y) -\lambda\cdot \nabla g(x,y)= 0,\\
    g(x,y)\leq 0,\\
    \lambda\geq 0,\\
    \lambda\cdot g(x,y)=0.
\end{cases}\]
These are known as the Karush-Kuhn-Tucker conditions for a problem with a single inequality constraint. The condition $\lambda\cdot g(x,y)=0$ ensures that either $g(x,y)=0$ and $\lambda$ can be positive, or $g(x,y)<0$, in which case the corresponding multiplier must be zero. In the latter case, the condition $\nabla f(x,y) -\lambda\cdot \nabla g(x,y)= 0$ reduces to $\nabla f(x,y)=0$, indicating that the point is a stationary point of the objective function in the interior of the feasible region.

\bigskip

\paragraph{\bf Multiple inequality constraints}

To finish, we extend to problems involving multiple inequality constraints. Suppose we want to minimize $f(x,y)$ subject to $g_i(x,y)\leq 0$, for $i=1,\ldots, M$. We then look for points $(x,y,\lambda_1,\ldots,\lambda_M)$ such that
\begin{equation}
    \label{KKT_conditions}
    \begin{cases}
    \nabla f(x,y) -\sum_{i=1}^M \lambda_i\cdot \nabla g_i(x,y)= 0,\\
    g_i(x,y)\leq 0\ \ \text{for all $i$},\\
    \lambda_i\geq 0\ \ \text{for all $i$},\\
    \lambda_i\cdot g(x,y)= 0\ \ \text{for all $i$.}
    \end{cases}
\end{equation}
These conditions provide a system of equations and inequalities whose solutions are candidates for constrained local minima or maxima of $f$.


\begin{thebibliography}{9}
    \bibitem{G}
    Gillis, N. (2021). Nonnegative matrix factorization. Society for Industrial and Applied Mathematics. https://lccn.loc.gov/2020042037
\end{thebibliography}

\end{document}